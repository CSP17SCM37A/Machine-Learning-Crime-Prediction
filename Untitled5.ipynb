{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "from IPython.display import Image\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from IPython.display import Image \n",
    "import pydotplus\n",
    "from pydotplus import graphviz\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positives instances in DataSet is 62.7195183141 %\n",
      "Negitives instances in DataSet is 37.2804816859 %\n"
     ]
    }
   ],
   "source": [
    "#1) Decision Trees\n",
    "\n",
    "#read given training dataset into dataframe\n",
    "data_input = pd.read_csv('C:\\Sai\\Machine Learning(CS584)\\project\\Crime Prediction Data\\Crime Prediction Data\\communities-crime-clean.csv')\n",
    "data_frame = pd.DataFrame(data_input)\n",
    "\n",
    "#1.a\n",
    "#New field HighCrime where voilentCrimePerPop is Greater than 0.1\n",
    "data_frame['HighCrime'] = np.where(data_frame['ViolentCrimesPerPop'] > 0.1,True,False)\n",
    "#count number of positives and negitives\n",
    "positive_negitive_counts = data_frame['HighCrime'].value_counts()\n",
    "positive_counts = positive_negitive_counts[True]\n",
    "negitive_counts = positive_negitive_counts[False]\n",
    "\n",
    "#count all number of columns\n",
    "total_counts = data_frame.HighCrime.count()\n",
    "\n",
    "positive_perc = positive_counts/total_counts\n",
    "negitive_perc = negitive_counts/total_counts\n",
    "\n",
    "print('Positives instances in DataSet is',positive_perc*100,'%')\n",
    "print('Negitives instances in DataSet is',negitive_perc*100,'%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "#method to split data data randomly based on features given\n",
    "#input: X(Set of features), y(class features)\n",
    "#output: X_train(train features),y_train(train class),X_eval(testing features),y_eval(test_class)\n",
    "def split_data_randomly(X,y):\n",
    "    #split train and test data randomly to calculate accuracy, precistion and recall\n",
    "    X_train, X_eval, y_train, y_eval = train_test_split( X, y, test_size=0.20, random_state=1 )\n",
    "    return X_train, X_eval, y_train, y_eval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#best features for clean dataset\n",
    "def find_best_features(data_input,list_a):\n",
    "    header_list = list(data_input)\n",
    "    feature_dict = {}\n",
    "    for i in range(0,len(list_a)):\n",
    "        feature_dict[list_a[i]] = i\n",
    "    feature_sorted_values = sorted(feature_dict.keys(),reverse=True)\n",
    "    count = 0\n",
    "    index_list = []\n",
    "    for each in feature_sorted_values:\n",
    "        if count == 10:\n",
    "            break;\n",
    "        count += 1\n",
    "        index_list.append(header_list[feature_dict[each]+3])\n",
    "    print(index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#method  to train and give accuracy precision recall values based on the input features and output class given\n",
    "def Decision_Tree_classfier(X,y):\n",
    "    #define a tree classifier\n",
    "    clf = tree.DecisionTreeClassifier()\n",
    "    #get X_train, y_train, X_eval, y_eval from split_data_randomly function\n",
    "    X_train, X_eval, y_train, y_eval = split_data_randomly( X, y)\n",
    "    #fit data to DecisionTree classifier\n",
    "    clf.fit(X_train,y_train.astype(int))\n",
    "    find_accuracy(X_train,X_eval,y_train,y_eval,clf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to find accuracy and metrics of the model\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def find_accuracy(X_train,X_eval,y_train,y_eval,clf):\n",
    "    y_pred = clf.predict(X_eval)\n",
    "    print('Cross val score: ',cross_val_score(clf, X_train, y_train.astype(int), cv=10),\"\\n\")\n",
    "    print('Accuracy is :: \\n')\n",
    "    print(metrics.accuracy_score(y_eval.astype(int), y_pred.astype(int)),\"\\n\")\n",
    "    print('\\n Confusion Matrix is:\\n')\n",
    "    print(metrics.confusion_matrix(y_eval.astype(int), y_pred.astype(int)),\"\\n\")\n",
    "    print('\\n Classification report is:\\n')\n",
    "    print(metrics.classification_report(y_eval.astype(int), y_pred.astype(int)),\"\\n\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross val score:  [ 0.6875      0.7125      0.7375      0.7375      0.76875     0.79375\n",
      "  0.7625      0.82911392  0.81012658  0.71518987] \n",
      "\n",
      "Accuracy is :: \n",
      "\n",
      "0.739348370927 \n",
      "\n",
      "\n",
      " Confusion Matrix is:\n",
      "\n",
      "[[ 98  58]\n",
      " [ 46 197]] \n",
      "\n",
      "\n",
      " Classification report is:\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.68      0.63      0.65       156\n",
      "          1       0.77      0.81      0.79       243\n",
      "\n",
      "avg / total       0.74      0.74      0.74       399\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1->b->1\n",
    "\n",
    "#get values ndarray form dataframe\n",
    "train_data_clean = data_frame.values\n",
    "train_data = train_data_clean.copy()\n",
    "#Split features into input and output\n",
    "#X: features 0,1,2 are non deterministic features, X is features from 3 to 102\n",
    "#(bcs 103 is ViolentCrimesperPop so exculed that column from both X and y)\n",
    "X = train_data_clean[:,3:102]\n",
    "#y: Decision class is HighCrimeRate we have created during 1.a\n",
    "y = train_data_clean[:,104]\n",
    "#calling Decision classifier function to get Accuracy,Precision and Recall of the model\n",
    "Decision_Tree_classfier(X,y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for Univariate feature selection\n",
      "\n",
      "Cross val score:  [ 0.73125     0.79375     0.7125      0.76875     0.79375     0.80625\n",
      "  0.76875     0.79746835  0.79113924  0.76582278] \n",
      "\n",
      "Accuracy is :: \n",
      "\n",
      "0.751879699248 \n",
      "\n",
      "\n",
      " Confusion Matrix is:\n",
      "\n",
      "[[101  55]\n",
      " [ 44 199]] \n",
      "\n",
      "\n",
      " Classification report is:\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.70      0.65      0.67       156\n",
      "          1       0.78      0.82      0.80       243\n",
      "\n",
      "avg / total       0.75      0.75      0.75       399\n",
      " \n",
      "\n",
      "Metrics for Recursive feature elimination feature selection\n",
      "\n",
      "Cross val score:  [ 0.75625     0.75625     0.79375     0.7875      0.74375     0.76875\n",
      "  0.7625      0.75316456  0.75949367  0.78481013] \n",
      "\n",
      "Accuracy is :: \n",
      "\n",
      "0.754385964912 \n",
      "\n",
      "\n",
      " Confusion Matrix is:\n",
      "\n",
      "[[108  48]\n",
      " [ 50 193]] \n",
      "\n",
      "\n",
      " Classification report is:\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.68      0.69      0.69       156\n",
      "          1       0.80      0.79      0.80       243\n",
      "\n",
      "avg / total       0.75      0.75      0.75       399\n",
      " \n",
      "\n",
      "Metrics for Principal Component Analysis feature selection\n",
      "\n",
      "Cross val score:  [ 0.6875      0.75        0.7125      0.7875      0.70625     0.7125\n",
      "  0.79375     0.78481013  0.80379747  0.67721519] \n",
      "\n",
      "Accuracy is :: \n",
      "\n",
      "0.701754385965 \n",
      "\n",
      "\n",
      " Confusion Matrix is:\n",
      "\n",
      "[[ 95  61]\n",
      " [ 58 185]] \n",
      "\n",
      "\n",
      " Classification report is:\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.62      0.61      0.61       156\n",
      "          1       0.75      0.76      0.76       243\n",
      "\n",
      "avg / total       0.70      0.70      0.70       399\n",
      " \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saich\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:70: DeprecationWarning: Function transform is deprecated; Support to use estimators as feature selectors will be removed in version 0.19. Use SelectFromModel instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for Feature Importance feature selection\n",
      "\n",
      "Cross val score:  [ 0.7625      0.775       0.7125      0.74375     0.79375     0.775\n",
      "  0.73125     0.76582278  0.79746835  0.75949367] \n",
      "\n",
      "Accuracy is :: \n",
      "\n",
      "0.744360902256 \n",
      "\n",
      "\n",
      " Confusion Matrix is:\n",
      "\n",
      "[[ 98  58]\n",
      " [ 44 199]] \n",
      "\n",
      "\n",
      " Classification report is:\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.69      0.63      0.66       156\n",
      "          1       0.77      0.82      0.80       243\n",
      "\n",
      "avg / total       0.74      0.74      0.74       399\n",
      " \n",
      "\n",
      "Most predictible features for Univariate feature selection: \n",
      "\n",
      "['racepctblack', 'PctIlleg', 'PctHousNoPhone', 'PctPersDenseHous', 'racePctHisp', 'pctWPubAsst', 'PctPopUnderPov', 'MedNumBR', 'PctNotSpeakEnglWell', 'PctKids2Par']\n",
      "\n",
      "Most predictible features for  Recursive Feature Elimination feature selection: \n",
      "\n",
      "['PctPersOwnOccup', 'PctRecImmig5', 'MedNumBR', 'PctRecImmig8', 'OwnOccLowQuart', 'RentMedian', 'MedOwnCostPctInc', 'PctRecImmig10', 'PctPopUnderPov', 'medFamInc']\n",
      "\n",
      "Most predictible features for  Principal Component Analysis feature selection: \n",
      "\n",
      "['PctSpeakEnglOnly', 'racePctWhite', 'PctHousOccup', 'pctUrban', 'PctYoungKids2Par', 'PctSameState85', 'PctSameCity85', 'PctKids2Par', 'PctFam2Par', 'PctBornSameState']\n",
      "\n",
      "Most predictible features for  Feature Importance feature selection: \n",
      "\n",
      "['PctYoungKids2Par', 'PctKids2Par', 'racePctHisp', 'FemalePctDiv', 'MalePctDivorce', 'pctWPubAsst', 'PctHousNoPhone', 'medIncome', 'PctTeen2Par', 'racePctWhite']\n"
     ]
    }
   ],
   "source": [
    "#1->b->2\n",
    "#feature selection methods \n",
    "#there are 4 different feature selection methods we have tried\n",
    "#  a)univariate selection\n",
    "#  b)Recursive Feature Elimination\n",
    "#  c)Principal Component Analysis\n",
    "#  d)Feature Importance\n",
    "\n",
    "#below are accuracy,precision and recall when we use different feature selection methods\n",
    "\n",
    "\n",
    "\n",
    "# a)univariate selection\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "#extract features\n",
    "univariate_clf = SelectKBest(score_func=chi2)\n",
    "fit = univariate_clf.fit(X,y.astype(int))\n",
    "#np.set_printoptions(precision=3)\n",
    "X_chi2 = fit.transform(X)\n",
    "#calling Decision classifier function to get Accuracy,Precision and Recall of the model\n",
    "print(\"Metrics for Univariate feature selection\\n\")\n",
    "Decision_Tree_classfier(X_chi2,y)\n",
    "\n",
    "\n",
    "\n",
    "# b)Recursive Feature Elimination\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "rfe = LogisticRegression()\n",
    "rfe_clf = RFE(rfe, 3)\n",
    "fit = rfe_clf.fit(X, y.astype(int))\n",
    "X_rfe = fit.transform(X)\n",
    "print(\"Metrics for Recursive feature elimination feature selection\\n\")\n",
    "Decision_Tree_classfier(X_rfe,y)\n",
    "\n",
    "\n",
    "# c)Principal Component Analysis\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=3)\n",
    "fit = pca.fit(X,y.astype(int))\n",
    "X_pca = fit.transform(X)\n",
    "print(\"Metrics for Principal Component Analysis feature selection\\n\")\n",
    "Decision_Tree_classfier(X_pca,y)\n",
    "\n",
    "\n",
    "\n",
    "# d)Feature Importance\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "fi = ExtraTreesClassifier()\n",
    "fi_clf = fi.fit(X, y.astype(int))\n",
    "X_fi = fi_clf.transform(X) \n",
    "print(\"Metrics for Feature Importance feature selection\\n\")\n",
    "Decision_Tree_classfier(X_fi,y)\n",
    "\n",
    "\n",
    "# 10 most predictive features when apply feature selection \n",
    "print('Most predictible features for Univariate feature selection: \\n')\n",
    "# a)univariate selection\n",
    "list_a = univariate_clf.scores_\n",
    "find_best_features(data_input,list_a)\n",
    "print('\\nMost predictible features for  Recursive Feature Elimination feature selection: \\n')\n",
    "# b)Recursive Feature Elimination\n",
    "list_a = rfe_clf.ranking_\n",
    "find_best_features(data_input,list_a)\n",
    "print('\\nMost predictible features for  Principal Component Analysis feature selection: \\n')\n",
    "# c)Principal Component Analysis\n",
    "list_a = pca.mean_\n",
    "find_best_features(data_input,list_a)\n",
    "print('\\nMost predictible features for  Feature Importance feature selection: \\n')\n",
    "# d)Feature Importance\n",
    "list_a = fi_clf.feature_importances_#rfe_clf.support_vectors_\n",
    "find_best_features(data_input,list_a)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#method cross_val_split will devide the data into X_CV_train, X_CV_eval, y_CV_train, y_CV_eval\n",
    "def cross_val_split_2(train_data_clean):\n",
    "    cv = KFold(n=len(train_data_clean),  \n",
    "           n_folds=10) \n",
    "    fold_accuracy = []\n",
    "    for train_fold, valid_fold in cv:\n",
    "        train = train_data_clean[train_fold] # Extract train data with cv indices\n",
    "        valid = train_data_clean[valid_fold] # Extract valid data with cv indices\n",
    "        #X: features 0,1,2 are non deterministic features, X is features from 3 to 102\n",
    "        #(bcs 103 is ViolentCrimesperPop so exculed that column from both X and y)\n",
    "        X_CV_train=train[:,3:102]\n",
    "        y_CV_train=train[:,104]\n",
    "        X_CV_eval=valid[:,3:102]\n",
    "        y_CV_eval=valid[:,104]\n",
    "    return X_CV_train,X_CV_eval,y_CV_train,y_CV_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_accuracy_CV(X_train,X_eval,y_train,y_eval,clf):\n",
    "    acc = 0.0\n",
    "    y_pred = clf.predict(X_eval)\n",
    "    print('Cross val score: ',cross_val_score(clf, X_train, y_train.astype(int), cv=10),\"\\n\")\n",
    "    print('Accuracy is :: \\n')\n",
    "    acc = metrics.accuracy_score(y_eval.astype(int), y_pred.astype(int))\n",
    "    print(acc,\"\\n\")\n",
    "    print('\\n Confusion Matrix is:\\n')\n",
    "    print(metrics.confusion_matrix(y_eval.astype(int), y_pred.astype(int)),\"\\n\")\n",
    "    print('\\n Classification report is:\\n')\n",
    "    print(metrics.classification_report(y_eval.astype(int), y_pred.astype(int)),\"\\n\")   \n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross val score:  [ 0.75        0.78333333  0.79444444  0.70555556  0.72222222  0.61111111\n",
      "  0.73184358  0.74860335  0.65730337  0.76966292] \n",
      "\n",
      "Accuracy is :: \n",
      "\n",
      "0.72864321608 \n",
      "\n",
      "\n",
      " Confusion Matrix is:\n",
      "\n",
      "[[69 38]\n",
      " [16 76]] \n",
      "\n",
      "\n",
      " Classification report is:\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.81      0.64      0.72       107\n",
      "          1       0.67      0.83      0.74        92\n",
      "\n",
      "avg / total       0.74      0.73      0.73       199\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1->c->1\n",
    "X_CV_train,X_CV_eval,y_CV_train,y_CV_eval = cross_val_split_2(train_data_clean)\n",
    "#Decision_Tree_classfier(X_CV_train,y_CV_train.astype(int))\n",
    "d_tree = tree.DecisionTreeClassifier()\n",
    "d_tree.fit(X_CV_train,y_CV_train.astype(int))\n",
    "DT_acc = find_accuracy_CV(X_CV_train,X_CV_eval,y_CV_train,y_CV_eval,d_tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_accuracy_2(X_train,X_eval,y_train,y_eval,clf):    \n",
    "    y_pred = clf.predict(X_eval)\n",
    "    #print(metrics.accuracy_score(y_eval.astype(int), y_pred.astype(int)),\"\\n\")    \n",
    "    return metrics.accuracy_score(y_eval.astype(int), y_pred.astype(int))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross val score:  [ 0.76666667  0.81666667  0.79444444  0.8         0.76111111  0.58333333\n",
      "  0.70949721  0.80446927  0.75280899  0.81460674] \n",
      "\n",
      "Accuracy is :: \n",
      "\n",
      "0.743718592965 \n",
      "\n",
      "\n",
      " Confusion Matrix is:\n",
      "\n",
      "[[82 25]\n",
      " [26 66]] \n",
      "\n",
      "\n",
      " Classification report is:\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.76      0.77      0.76       107\n",
      "          1       0.73      0.72      0.72        92\n",
      "\n",
      "avg / total       0.74      0.74      0.74       199\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#2->a->1 Linear Classification\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "#Gaussian_clf.fit(X_CV_train, y_CV_train)\n",
    "X_CV_train,X_CV_eval,y_CV_train,y_CV_eval = cross_val_split_2(train_data_clean)\n",
    "Gaussian_clf = GaussianNB()\n",
    "Gaussian_clf.fit(X_CV_train,y_CV_train.astype(int))\n",
    "GNB_acc = find_accuracy_CV(X_CV_train,X_CV_eval,y_CV_train,y_CV_eval,Gaussian_clf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Most Predictive features with Gaussian NB \n",
      "\n",
      "PctLargHouseOccup\n",
      "PctVacMore6Mos\n",
      "agePct12t21\n",
      "PctUsePubTrans\n",
      "PctWorkMom\n",
      "PopDens\n",
      "PctWorkMomYoungKids\n",
      "PctTeen2Par\n",
      "NumIlleg\n",
      "PctSameState85\n"
     ]
    }
   ],
   "source": [
    "# 2->a->2\n",
    "import math\n",
    "feature_dict = {}\n",
    "for i in range(3,102):\n",
    "    ar = train_data_clean[:,i]\n",
    "    true_count = 0\n",
    "    true_values = []\n",
    "    false_count = 0\n",
    "    false_values = []\n",
    "    for each in ar:\n",
    "        if each > 0.1:\n",
    "            true_count += 1\n",
    "            true_values.append(each)\n",
    "        else:\n",
    "            false_count +=1\n",
    "            false_values.append(each)\n",
    "    mean_ture = true_count/len(ar)\n",
    "    mean_false = false_count/len(ar)\n",
    "    true_sd = 0\n",
    "    false_sd = 0\n",
    "    for each in true_values:\n",
    "        true_sd = true_sd + ((each - mean_ture) * (each - mean_ture))\n",
    "    for each in false_values:\n",
    "        false_sd = false_sd + ((each - mean_false) * (each - mean_false))\n",
    "    true_sd = math.sqrt(true_sd/true_count)\n",
    "    false_sd = math.sqrt(false_sd/false_count)\n",
    "    predict_value = abs(mean_ture-mean_false)/(true_sd+false_sd)\n",
    "    feature_dict[predict_value] = i\n",
    "\n",
    "arr = sorted(feature_dict.keys(),reverse=True)\n",
    "count = 0\n",
    "index_list = []\n",
    "for each in arr:\n",
    "    if count == 10:\n",
    "        break\n",
    "    count += 1\n",
    "    #print(feature_dict[each])\n",
    "    index_list.append(feature_dict[each])\n",
    "header_list = list(data_input)\n",
    "print('\\n Most Predictive features with Gaussian NB \\n')\n",
    "for each in index_list:\n",
    "    print(header_list[each+3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For Decision Tree Accuracy is :: 0.72864321608\n",
      "For Guassian NB Accuracy is :: 0.743718592965\n",
      "Therefore Gaussian NB has Higher Accuracy than Decision Tree\n"
     ]
    }
   ],
   "source": [
    "#2->1->3\n",
    "print('\\nFor Decision Tree Accuracy is ::', DT_acc)\n",
    "print('For Guassian NB Accuracy is ::', GNB_acc)\n",
    "print('Therefore Gaussian NB has Higher Accuracy than Decision Tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "def cross_val_split(clf,train_data_clean):\n",
    "    cv = KFold(n=len(train_data_clean),  \n",
    "           n_folds=10) \n",
    "    fold_accuracy = []\n",
    "    for train_fold, valid_fold in cv:\n",
    "        train = train_data_clean[train_fold] # Extract train data with cv indices\n",
    "        valid = train_data_clean[valid_fold] # Extract valid data with cv indices\n",
    "        X_CV_train=train[:,3:103]\n",
    "        y_CV_train=train[:,104]\n",
    "        X_CV_eval=valid[:,3:103]\n",
    "        y_CV_eval=valid[:,104]\n",
    "        model = clf.fit(X = X_CV_train, y = y_CV_train.astype(int))\n",
    "        valid_acc = model.score(X = X_CV_eval, \n",
    "                                y = y_CV_eval.astype(int))\n",
    "        fold_accuracy.append(valid_acc)    \n",
    "        find_accuracy(X_CV_train,X_CV_eval,y_CV_train,y_CV_eval,clf)\n",
    "    #print(\"Accuracy per fold: \", fold_accuracy, \"\\n\")\n",
    "    print(\"Average accuracy: \", sum(fold_accuracy)/len(fold_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross val score:  [ 0.75555556  0.88888889  0.85        0.81666667  0.75555556  0.62777778\n",
      "  0.70391061  0.83240223  0.85955056  0.84269663] \n",
      "\n",
      "Accuracy is :: \n",
      "\n",
      "0.809045226131 \n",
      "\n",
      "\n",
      " Confusion Matrix is:\n",
      "\n",
      "[[79 28]\n",
      " [10 82]] \n",
      "\n",
      "\n",
      " Classification report is:\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.74      0.81       107\n",
      "          1       0.75      0.89      0.81        92\n",
      "\n",
      "avg / total       0.82      0.81      0.81       199\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#2->b->1\n",
    "\n",
    "#Linear SVC\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "X_CV_train,X_CV_eval,y_CV_train,y_CV_eval = cross_val_split_2(train_data_clean)\n",
    "SVC_clf = SVC(kernel='linear')\n",
    "SVC_clf.fit(X_CV_train,y_CV_train.astype(int))\n",
    "LinearSVC_acc = find_accuracy_CV(X_CV_train,X_CV_eval,y_CV_train,y_CV_eval,SVC_clf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Most Predictive features with Linear SVC: \n",
      "\n",
      "racepctblack\n",
      "agePct12t21\n",
      "MalePctDivorce\n",
      "population\n",
      "PersPerOccupHous\n",
      "agePct65up\n",
      "racePctHisp\n",
      "TotalPctDiv\n",
      "RentHighQ\n",
      "PctSpeakEnglOnly\n"
     ]
    }
   ],
   "source": [
    "#2->b->2\n",
    "weights = SVC_clf.coef_[0]\n",
    "weight_dict = {}\n",
    "for i in range(0,len(weights)):\n",
    "    weight_dict[weights[i]] = i\n",
    "weight_val = sorted(weight_dict.keys(),reverse=True)\n",
    "count = 0\n",
    "index_list2 = []\n",
    "for each in weight_val:\n",
    "    if count == 10:\n",
    "        break\n",
    "    count += 1\n",
    "    index_list2.append(weight_dict[each])\n",
    "print('\\n Most Predictive features with Linear SVC: \\n')\n",
    "for each in index_list2:\n",
    "    print(header_list[each+3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For Decision Tree Accuracy is :: 0.72864321608\n",
      "For Linear SVC Accuracy is :: 0.809045226131\n",
      "\n",
      "Therefore Linear SVC has Higher Accuracy than Decision Tree\n"
     ]
    }
   ],
   "source": [
    "#2->b->3\n",
    "print('\\nFor Decision Tree Accuracy is ::', DT_acc)\n",
    "print('For Linear SVC Accuracy is ::', LinearSVC_acc)\n",
    "print('\\nTherefore Linear SVC has Higher Accuracy than Decision Tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "def cross_val_split_regr(train_data_clean):\n",
    "    cv = KFold(n=len(train_data_clean),  \n",
    "           n_folds=10) \n",
    "    fold_accuracy = []\n",
    "    for train_fold, valid_fold in cv:\n",
    "        train = train_data_clean[train_fold] # Extract train data with cv indices\n",
    "        valid = train_data_clean[valid_fold] # Extract valid data with cv indices\n",
    "        X_CV_train=train[:,3:102]\n",
    "        y_CV_train=train[:,103]\n",
    "        X_CV_eval=valid[:,3:102]\n",
    "        y_CV_eval=valid[:,103]\n",
    "    return X_CV_train,X_CV_eval,y_CV_train,y_CV_eval    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error for 10 fold cross validation : 0.04\n",
      "Mean squared error when training set and testing set are same : 0.08\n",
      "\n",
      " Most predictive features: \n",
      "\n",
      "PctLargHouseFam\n",
      "NumIlleg\n",
      "PctKids2Par\n",
      "OwnOccHiQuart\n",
      "PctPersDenseHous\n",
      "PctRecImmig10\n",
      "MalePctDivorce\n",
      "PersPerOwnOccHous\n",
      "MalePctNevMarr\n",
      "PersPerOccupHous\n",
      "\n",
      "Least predictive features: \n",
      "\n",
      "PctLargHouseOccup\n",
      "population\n",
      "PersPerFam\n",
      "perCapInc\n",
      "PctFam2Par\n",
      "agePct16t24\n",
      "OwnOccLowQuart\n",
      "TotalPctDiv\n",
      "PctRecImmig5\n",
      "pctWWage\n"
     ]
    }
   ],
   "source": [
    "#3.a.1) LinearRegression\n",
    "\n",
    "from sklearn import datasets, linear_model\n",
    "\n",
    "regr_clf = linear_model.LinearRegression()\n",
    "X_CV_train,X_CV_eval,y_CV_train,y_CV_eval = cross_val_split_regr(train_data_clean)\n",
    "regr_clf.fit(X = X_CV_train, y = y_CV_train.astype(int))\n",
    "print(\"Mean squared error for 10 fold cross validation : %.2f\"\n",
    "      % np.mean((regr_clf.predict(X_CV_eval) - y_CV_eval) ** 2))\n",
    "    \n",
    "#a.2) MSE on the training set (train on all the data then test on it all\n",
    "X_regr_train = train_data[:,3:102]\n",
    "y_regr_train = train_data[:,103]\n",
    "regr_clf_2 = linear_model.LinearRegression()\n",
    "regr_clf_2.fit(X_regr_train,y_regr_train.astype(int))\n",
    "print(\"Mean squared error when training set and testing set are same : %.2f\"\n",
    "      % np.mean((regr_clf_2.predict(X_regr_train) - y_regr_train) ** 2))\n",
    "\n",
    "regr_coef = []\n",
    "regr_coef = regr_clf.coef_\n",
    "coef_dict = {}\n",
    "for i in range(0,len(regr_coef)):\n",
    "    coef_dict[regr_coef[i]] = i\n",
    "coef_val_dec = sorted(coef_dict.keys(),reverse=True)\n",
    "print('\\n Most predictive features: \\n')\n",
    "count = 0;\n",
    "index_list_1 = []\n",
    "for each in coef_val_dec:\n",
    "    if(count == 10):\n",
    "        break\n",
    "    count += 1\n",
    "    index_list_1.append(coef_dict[each])\n",
    "for each in index_list_1:\n",
    "    print(header_list[each+3])\n",
    "\n",
    "coef_val_asc = sorted(coef_dict.keys())\n",
    "print('\\nLeast predictive features: \\n')\n",
    "count = 0\n",
    "index_list_2 = []\n",
    "for each in coef_val_asc:\n",
    "    if(count == 10):\n",
    "        break\n",
    "    count += 1\n",
    "    index_list_2.append(coef_dict[each])\n",
    "for each in index_list_2:\n",
    "    print(header_list[each+3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error for 10 fold cross validation: 0.04\n",
      "Best Alpha : 10.0 \n",
      "\n",
      "Mean squared error when training set and testing set are same : 0.02\n",
      "Best Alpha :  1.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#3.b.1)\n",
    "\n",
    "from sklearn import linear_model \n",
    "\n",
    "ridge_clf = linear_model.RidgeCV(alphas=(10,1.0,0.1,0.01,0.001))\n",
    "X_CV_train,X_CV_eval,y_CV_train,y_CV_eval = cross_val_split_regr(train_data_clean)\n",
    "ridge_clf.fit(X = X_CV_train, y = y_CV_train.astype(int))\n",
    "print(\"Mean squared error for 10 fold cross validation: %.2f\" % np.mean((regr_clf_2.predict(X_CV_eval) - y_CV_eval) ** 2))\n",
    "print('Best Alpha :', ridge_clf.alpha_,'\\n')\n",
    "    \n",
    "\n",
    "#a.2) MSE on the training set (train on all the data then test on it all\n",
    "X_ridge_train = train_data[:,3:102]\n",
    "y_ridge_train = train_data[:,103]\n",
    "ridge_clf_2 = linear_model.RidgeCV(alphas=(10,1.0,0.1,0.01,0.001))\n",
    "ridge_clf_2.fit(X_ridge_train,y_ridge_train)\n",
    "print(\"Mean squared error when training set and testing set are same : %.2f\"\n",
    "      % np.mean((ridge_clf_2.predict(X_ridge_train) - y_ridge_train) ** 2))\n",
    "print('Best Alpha : ', ridge_clf_2.alpha_,'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 0.15\n",
      "Mean squared error when training set and testing set are same : 0.02\n"
     ]
    }
   ],
   "source": [
    "#3->c\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly_clf = PolynomialFeatures(degree=2)\n",
    "X_CV_train,X_CV_eval,y_CV_train,y_CV_eval = cross_val_split_regr(train_data_clean)\n",
    "X_ = poly_clf.fit_transform(X_CV_train)\n",
    "predict_ = poly_clf.fit_transform(X_CV_eval)\n",
    "#poly_clf.fit(X_CV_train,y_CV_train)\n",
    "#print(\"Mean squared error: %.2f\"\n",
    "#      % np.mean((poly_clf.predict(X_CV_eval) - y_CV_eval) ** 2))\n",
    "\n",
    "\n",
    "\n",
    "linear_clf = linear_model.LinearRegression()\n",
    "linear_clf.fit(X_, y_CV_train)\n",
    "#print(linear_clf.predict(predict_))\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % np.mean((linear_clf.predict(predict_) - y_CV_eval) ** 2))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#3->c->2 on the training set (train on all the data then test on it all\n",
    "\n",
    "linear_clf_2 = linear_model.LinearRegression()\n",
    "linear_clf_2.fit(X_ridge_train,y_ridge_train)\n",
    "print(\"Mean squared error when training set and testing set are same : %.2f\"\n",
    "      % np.mean((ridge_clf_2.predict(X_ridge_train) - y_ridge_train) ** 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for linear model is lesser than Quadratic model when degree = 2\n",
      "\n",
      "MSE for linear model is greater than Quadratic model when degree = 3 but time complexity wil be more and program is taking more and more time to run\n",
      "\n",
      "therefore when degree of the quadratic model is increasing MSE is decreasing\n"
     ]
    }
   ],
   "source": [
    "#3->c->3\n",
    "\n",
    "print('MSE for linear model is lesser than Quadratic model when degree = 2\\n')\n",
    "print('MSE for linear model is greater than Quadratic model when degree = 3 but time complexity wil be more and program is taking more and more time to run\\n')\n",
    "print('therefore when degree of the quadratic model is increasing MSE is decreasing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "def cross_val_split_full(train_data_full):\n",
    "    cv = KFold(n=len(train_data_full),  \n",
    "           n_folds=10) \n",
    "    fold_accuracy = []\n",
    "    for train_fold, valid_fold in cv:\n",
    "        train = train_data_full[train_fold] # Extract train data with cv indices\n",
    "        valid = train_data_full[valid_fold] # Extract valid data with cv indices\n",
    "        X_CV_train=train[:,3:124]\n",
    "        y_CV_train=train[:,125]\n",
    "        X_CV_eval=valid[:,3:124]\n",
    "        y_CV_eval=valid[:,125]\n",
    "    return X_CV_train,X_CV_eval,y_CV_train,y_CV_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross val score:  [ 1.          1.          1.          0.99444444  1.          1.          1.\n",
      "  1.          1.          1.        ] \n",
      "\n",
      "Accuracy from metrics\n",
      "Accuracy is ::  0.994974874372 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#DirtyData\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "data_input_full = pd.read_csv('C:\\Sai\\Machine Learning(CS584)\\project\\Crime Prediction Data\\Crime Prediction Data\\communities-crime-full.csv')\n",
    "data_frame_full_data = pd.DataFrame(data_input_full)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "colums_with_missing_vlaues = ['county','community','OtherPerCap','LemasSwornFT','LemasSwFTPerPop','LemasSwFTFieldOps','LemasSwFTFieldPerPop','LemasTotalReq','LemasTotReqPerPop','PolicReqPerOffic','PolicPerPop','RacialMatchCommPol','PctPolicWhite','PctPolicBlack','PctPolicHisp','PctPolicAsian','PctPolicMinor','OfficAssgnDrugUnits','NumKindsDrugsSeiz','PolicAveOTWorked','PolicCars','PolicOperBudg','LemasPctPolicOnPatr','LemasGangUnitDeploy','PolicBudgPerPop']\n",
    "data_frame_full_rev = data_frame_full_data\n",
    "del data_frame_full_rev['communityname']\n",
    "\n",
    "for value in colums_with_missing_vlaues:\n",
    "    #print(value)\n",
    "    data_frame_full_rev[value].replace(['?'], ['NaN'], inplace='True')\n",
    "\n",
    "imp = Imputer(missing_values='NaN', strategy = 'mean', axis=0)\n",
    "x = pd.DataFrame(data_frame_full_rev.values[:,:-1].astype(float))\n",
    "imp.fit(x)\n",
    "dataset_replaced_values = pd.DataFrame(imp.transform(x))\n",
    "\n",
    "dataset_replaced_values[125] = np.where(dataset_replaced_values[124] > 0.1,True,False)\n",
    "train_data_full = dataset_replaced_values.values\n",
    "dirtydata_clf = tree.DecisionTreeClassifier()\n",
    "X_CV_train,X_CV_eval,y_CV_train,y_CV_eval = cross_val_split_full(train_data_full)\n",
    "dirtydata_clf.fit(X_CV_train,y_CV_train.astype(int))\n",
    "find_accuracy(X_CV_train,X_CV_eval,y_CV_train,y_CV_eval,dirtydata_clf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "def cross_val_split_full_regr(train_data_full):\n",
    "    cv = KFold(n=len(train_data_full),  \n",
    "           n_folds=10) \n",
    "    fold_accuracy = []\n",
    "    for train_fold, valid_fold in cv:\n",
    "        train = train_data_full[train_fold] # Extract train data with cv indices\n",
    "        valid = train_data_full[valid_fold] # Extract valid data with cv indices\n",
    "        X_CV_train=train[:,3:123]\n",
    "        y_CV_train=train[:,124]\n",
    "        X_CV_eval=valid[:,3:123]\n",
    "        y_CV_eval=valid[:,124]\n",
    "    return X_CV_train,X_CV_eval,y_CV_train,y_CV_eval    \n",
    "    #model = clf.fit(X = X_CV_train, y = y_CV_train.astype(int))\n",
    "    #valid_acc = model.score(X = X_CV_eval,y = y_CV_eval.astype(int))\n",
    "    #fold_accuracy.append(valid_acc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross val score:  [ 0.88888889  0.79444444  0.9         0.76666667  0.74444444  0.74860335\n",
      "  0.82681564  0.8547486   0.79213483  0.82022472] \n",
      "\n",
      "Accuracy from metrics\n",
      "Accuracy is ::  0.845 \n",
      "\n",
      "Cross val score:  [ 0.83333333  0.8         0.89444444  0.75        0.75555556  0.75977654\n",
      "  0.81564246  0.8547486   0.78089888  0.83707865] \n",
      "\n",
      "Accuracy from metrics\n",
      "Accuracy is ::  0.895 \n",
      "\n",
      "Cross val score:  [ 0.73480663  0.83333333  0.89385475  0.77653631  0.70391061  0.74860335\n",
      "  0.83798883  0.84916201  0.7877095   0.81564246] \n",
      "\n",
      "Accuracy from metrics\n",
      "Accuracy is ::  0.94 \n",
      "\n",
      "Cross val score:  [ 0.74585635  0.91666667  0.88888889  0.91061453  0.73184358  0.73743017\n",
      "  0.83240223  0.84357542  0.7877095   0.82681564] \n",
      "\n",
      "Accuracy from metrics\n",
      "Accuracy is ::  0.72864321608 \n",
      "\n",
      "Cross val score:  [ 0.73888889  0.9         0.84444444  0.93333333  0.80555556  0.69832402\n",
      "  0.82122905  0.83240223  0.7877095   0.84269663] \n",
      "\n",
      "Accuracy from metrics\n",
      "Accuracy is ::  0.72864321608 \n",
      "\n",
      "Cross val score:  [ 0.74444444  0.91111111  0.85        0.9         0.78888889  0.67597765\n",
      "  0.82681564  0.82681564  0.79888268  0.8258427 ] \n",
      "\n",
      "Accuracy from metrics\n",
      "Accuracy is ::  0.819095477387 \n",
      "\n",
      "Cross val score:  [ 0.73888889  0.88888889  0.84444444  0.88333333  0.81111111  0.67222222\n",
      "  0.72222222  0.83707865  0.76404494  0.80337079] \n",
      "\n",
      "Accuracy from metrics\n",
      "Accuracy is ::  0.874371859296 \n",
      "\n",
      "Cross val score:  [ 0.74444444  0.9         0.83333333  0.90555556  0.78333333  0.71666667\n",
      "  0.74444444  0.87640449  0.78089888  0.80898876] \n",
      "\n",
      "Accuracy from metrics\n",
      "Accuracy is ::  0.793969849246 \n",
      "\n",
      "Cross val score:  [ 0.73333333  0.88333333  0.87777778  0.86666667  0.8         0.7\n",
      "  0.72222222  0.87078652  0.76404494  0.8258427 ] \n",
      "\n",
      "Accuracy from metrics\n",
      "Accuracy is ::  0.844221105528 \n",
      "\n",
      "Cross val score:  [ 0.74444444  0.89444444  0.83888889  0.87777778  0.78333333  0.72777778\n",
      "  0.69832402  0.81564246  0.85955056  0.83146067] \n",
      "\n",
      "Accuracy from metrics\n",
      "Accuracy is ::  0.753768844221 \n",
      "\n",
      "Average accuracy:  0.822271356784\n",
      "['pctUrban', 'PctSpeakEnglOnly', 'PctHousOccup', 'MedYrHousBuilt', 'PctHousOwnOcc', 'PctFam2Par', 'PctKids2Par', 'pctWWage', 'PctBornSameState', 'PctEmploy']\n"
     ]
    }
   ],
   "source": [
    "#Teams \n",
    "#a.\tIf you are working in a team of two people:\n",
    "#i.\tExperiment with two learning methods other than those described above \n",
    "#(one can be a non-linear kernel for SVM) for the classification problem, explaining clearly what you did. \n",
    "#Show CV results for both the clean and full datasets.\n",
    "\n",
    "\n",
    "#non linear svm for clean dataset\n",
    "from sklearn import  svm\n",
    "non_linear_svm_clean_clf=svm.SVC(kernel='rbf')\n",
    "cross_val_split(non_linear_svm_clean_clf,train_data_clean)\n",
    "#non_linear_svm_clean_clf.fit(X_CV_train, y_CV_train.astype(int))\n",
    "#find_accuracy(X_CV_train,X_CV_eval,y_CV_train,y_CV_eval,non_linear_svm_clean_clf)\n",
    "w = non_linear_svm_clean_clf.support_vectors_\n",
    "find_features_non_linear_svm(non_linear_svm_clean_clf,data_input,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#best features for clean dataset\n",
    "def find_features_non_linear_svm(clf,data_input,w):\n",
    "    #w = clf.support_vectors_\n",
    "    list_a = w[0]\n",
    "    #print(list(w,list(zip(w,features)))\n",
    "    header_list = list(data_input)\n",
    "    svm_clean_dict = {}\n",
    "    for i in range(0,len(list_a)):\n",
    "        svm_clean_dict[list_a[i]] = i\n",
    "    svm_sorted_values = sorted(svm_clean_dict.keys(),reverse=True)\n",
    "    count = 0\n",
    "    index_list = []\n",
    "    for each in svm_sorted_values:\n",
    "        if count == 10:\n",
    "            break;\n",
    "        count += 1\n",
    "        index_list.append(header_list[svm_clean_dict[each]+3])\n",
    "    print(index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross val score:  [ 0.85555556  0.86666667  0.87777778  0.87777778  0.86666667  0.87222222\n",
      "  0.87777778  0.87150838  0.87640449  0.85955056] \n",
      "\n",
      "Accuracy from metrics\n",
      "Accuracy is ::  0.86432160804 \n",
      "\n",
      "['TotalPctDiv', 'LemasSwFTFieldOps', 'FemalePctDiv', 'PctSpeakEnglOnly', 'PctHousLess3BR', 'PctHousOccup', 'racePctWhite', 'PctEmploy', 'PctPolicWhite', 'pctWWage']\n"
     ]
    }
   ],
   "source": [
    "#non linear svm for clean dataset\n",
    "from sklearn import  svm\n",
    "non_linear_svm_full_clf=svm.SVC(kernel='rbf')\n",
    "X_CV_train,X_CV_eval,y_CV_train,y_CV_eval = cross_val_split_full(train_data_full)\n",
    "non_linear_svm_full_clf.fit(X_CV_train, y_CV_train.astype(int))\n",
    "find_accuracy(X_CV_train,X_CV_eval,y_CV_train,y_CV_eval,non_linear_svm_full_clf)\n",
    "w = non_linear_svm_full_clf.support_vectors_\n",
    "find_features_non_linear_svm(non_linear_svm_full_clf,data_input_full,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross val score:  [ 0.81666667  0.82777778  0.83888889  0.72222222  0.56666667  0.73743017\n",
      "  0.68156425  0.73743017  0.80337079  0.74157303] \n",
      "\n",
      "Accuracy from metrics\n",
      "Accuracy is ::  0.86 \n",
      "\n",
      "Cross val score:  [ 0.83888889  0.87777778  0.66666667  0.72777778  0.57777778  0.69832402\n",
      "  0.77094972  0.62011173  0.80898876  0.83707865] \n",
      "\n",
      "Accuracy from metrics\n",
      "Accuracy is ::  0.885 \n",
      "\n",
      "Cross val score:  [ 0.74033149  0.71111111  0.72067039  0.75418994  0.67597765  0.51955307\n",
      "  0.65363128  0.84357542  0.78212291  0.81005587] \n",
      "\n",
      "Accuracy from metrics\n",
      "Accuracy is ::  0.895 \n",
      "\n",
      "Cross val score:  [ 0.78453039  0.64444444  0.74444444  0.91061453  0.72625698  0.67039106\n",
      "  0.79888268  0.65921788  0.62569832  0.81005587] \n",
      "\n",
      "Accuracy from metrics\n",
      "Accuracy is ::  0.678391959799 \n",
      "\n",
      "Cross val score:  [ 0.68888889  0.93333333  0.7         0.83333333  0.76111111  0.51396648\n",
      "  0.72625698  0.66480447  0.79888268  0.81460674] \n",
      "\n",
      "Accuracy from metrics\n",
      "Accuracy is ::  0.733668341709 \n",
      "\n",
      "Cross val score:  [ 0.71111111  0.8         0.85        0.79444444  0.76666667  0.62011173\n",
      "  0.84916201  0.79888268  0.77653631  0.80898876] \n",
      "\n",
      "Accuracy from metrics\n",
      "Accuracy is ::  0.738693467337 \n",
      "\n",
      "Cross val score:  [ 0.74444444  0.78888889  0.65        0.82222222  0.77222222  0.54444444\n",
      "  0.74444444  0.66853933  0.69662921  0.76966292] \n",
      "\n",
      "Accuracy from metrics\n",
      "Accuracy is ::  0.824120603015 \n",
      "\n",
      "Cross val score:  [ 0.79444444  0.79444444  0.88888889  0.86111111  0.75        0.65\n",
      "  0.69444444  0.73033708  0.79775281  0.76966292] \n",
      "\n",
      "Accuracy from metrics\n",
      "Accuracy is ::  0.768844221106 \n",
      "\n",
      "Cross val score:  [ 0.76666667  0.77777778  0.77222222  0.77777778  0.72222222  0.67222222\n",
      "  0.71666667  0.78089888  0.70786517  0.84269663] \n",
      "\n",
      "Accuracy from metrics\n",
      "Accuracy is ::  0.693467336683 \n",
      "\n",
      "Cross val score:  [ 0.68333333  0.82777778  0.83333333  0.82222222  0.73333333  0.51666667\n",
      "  0.72067039  0.82681564  0.87640449  0.80898876] \n",
      "\n",
      "Accuracy from metrics\n",
      "Accuracy is ::  0.824120603015 \n",
      "\n",
      "Average accuracy:  0.790130653266\n",
      "['racepctblack', 'MalePctDivorce', 'racePctHisp', 'TotalPctDiv', 'FemalePctDiv', 'PctIlleg', 'HousVacant', 'pctUrban', 'PersPerRentOccHous', 'PctPersDenseHous']\n"
     ]
    }
   ],
   "source": [
    "#SGD Classifier using clean data\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "SGD_clean_clf = SGDClassifier(loss=\"hinge\", penalty=\"l2\")\n",
    "cross_val_split(SGD_clean_clf,train_data_clean)\n",
    "w = SGD_clean_clf.coef_\n",
    "find_features_non_linear_svm(SGD_clean_clf,data_input,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross val score:  [ 0.90555556  0.95        0.90555556  0.88888889  0.87222222  0.88333333\n",
      "  0.92222222  0.89385475  0.88202247  0.93820225] \n",
      "\n",
      "Accuracy from metrics\n",
      "Accuracy is ::  0.934673366834 \n",
      "\n",
      "['householdsize', 'PctOccupMgmtProf', 'racePctAsian', 'FemalePctDiv', 'MalePctNevMarr', 'NumIlleg', 'MedNumBR', 'numbUrban', 'PersPerOwnOccHous', 'PctPersOwnOccup']\n"
     ]
    }
   ],
   "source": [
    "#SGD Classifier using clean data\n",
    "\n",
    "SGD_full_clf = SGDClassifier(loss=\"hinge\", penalty=\"l2\")\n",
    "X_CV_train,X_CV_eval,y_CV_train,y_CV_eval = cross_val_split_full(train_data_full)\n",
    "SGD_full_clf.fit(X_CV_train, y_CV_train.astype(int))\n",
    "find_accuracy(X_CV_train,X_CV_eval,y_CV_train,y_CV_eval,SGD_full_clf)\n",
    "w = SGD_clean_clf.coef_\n",
    "find_features_non_linear_svm(SGD_full_clf,data_input_full,w)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "def find_mean_median_mode(arr):\n",
    "    #arr = data_input.values[:,103]\n",
    "    list_mean_mode_median = []\n",
    "    list_mean_mode_median.append(np.mean(arr))\n",
    "    list_mean_mode_median.append(np.median(arr))\n",
    "    list_mean_mode_median.append(mode(arr))\n",
    "    #list_mean_mode_median.append(np.average(arr))\n",
    "    return list_mean_mode_median\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum accuracy threshold for Decistion Tree :  0.35  Maximum accuracy:  0.743718592965\n",
      "Maximum accuracy threshold for Guassian NB :  0.01  Maximum accuracy:  0.743718592965\n",
      "Maximum accuracy threshold for Linear SVC :  0.01  Maximum accuracy:  0.809045226131\n"
     ]
    }
   ],
   "source": [
    "# 5.b\n",
    "#ii.\tDevise a method to find the most useful threshold for dividing high crime areas from low crime areas (i.e., discretizing XXX to compute highCrime). Define clearly what you mean by useful.\n",
    "#iii.\tShow CV results for both the clean and full datasets for at least three different classification methods above.\n",
    "#iv.\tHow are these results similar and different from the previous results (with a fixed threshold of 0.1). What does this say about how to approach such a problem.\n",
    "\n",
    "\n",
    "#clean data\n",
    "\n",
    "arr = data_input.values[:,103]\n",
    "list_mean_median_mode = find_mean_median_mode(arr)\n",
    "data_frame_c = data_frame.copy()\n",
    "#print(list_mean_median_mode)\n",
    "val = 0.01\n",
    "acc = 0.0\n",
    "DT_max_acc = 0.0\n",
    "DT_max_val = 0.0\n",
    "GD_max_acc = 0.0\n",
    "GD_max_val = 0.0\n",
    "SVC_max_acc = 0.0\n",
    "SVC_max_val = 0.0\n",
    "while val < 1.0:\n",
    "    data_frame_c['HighCrime'] = np.where(data_frame_c['ViolentCrimesPerPop'] > val,True,False)\n",
    "    train_data_clean_1 = data_frame_c.values\n",
    "    X_CV_train,X_CV_eval,y_CV_train,y_CV_eval = cross_val_split_2(train_data_clean_1)\n",
    "    \n",
    "    #Decision Tree classfier\n",
    "    DTree_clf=tree.DecisionTreeClassifier()\n",
    "    DTree_clf.fit(X_CV_train,y_CV_train.astype(int))\n",
    "    acc = find_accuracy_2(X_CV_train,X_CV_eval,y_CV_train,y_CV_eval,DTree_clf)\n",
    "    if acc > DT_max_acc:\n",
    "        DT_max_val = val\n",
    "        DT_max_acc = acc\n",
    "    \n",
    "    #GaussianNB classfier\n",
    "    Gaussian_clf_1 = GaussianNB()\n",
    "    Gaussian_clf_1.fit(X_CV_train,y_CV_train.astype(int))\n",
    "    acc = find_accuracy_2(X_CV_train,X_CV_eval,y_CV_train,y_CV_eval,Gaussian_clf_1)\n",
    "    if acc > GD_max_acc:\n",
    "        GD_max_val = val\n",
    "        GD_max_acc = acc\n",
    "    \n",
    "    #Linear SVC\n",
    "        SVC_clf_1 = SVC(kernel='linear')\n",
    "    SVC_clf_1.fit(X_CV_train,y_CV_train.astype(int))\n",
    "    acc = find_accuracy_2(X_CV_train,X_CV_eval,y_CV_train,y_CV_eval,SVC_clf_1)\n",
    "    if acc > SVC_max_acc:\n",
    "        SVC_max_val = val\n",
    "        SVC_max_acc = acc\n",
    "    del data_frame_c['HighCrime']\n",
    "    #del train_data_clean_1['HighCrime']\n",
    "    train_data_clean_1 = []\n",
    "    val += 0.02\n",
    "    \n",
    "print('Maximum accuracy threshold for Decistion Tree : ',round(DT_max_val,2), ' Maximum accuracy: ', DT_max_acc)\n",
    "print('Maximum accuracy threshold for Guassian NB : ',round(GD_max_val,2), ' Maximum accuracy: ', GD_max_acc)\n",
    "print('Maximum accuracy threshold for Linear SVC : ',round(SVC_max_val,2), ' Maximum accuracy: ', SVC_max_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0.14' 'NaN' 'NaN' ..., '0.28' '0.18' '0.13']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown label type: 'unknown'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-371-7e3cdf1b34e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[1;31m#Decision Tree classfier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mDTree_clf_full\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDecisionTreeClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mDTree_clf_full\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_CV_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_CV_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0macc_full\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_accuracy_2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_CV_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_CV_eval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_CV_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_CV_eval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mDTree_clf_full\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0macc_full\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mDT_max_acc_full\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\saich\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    737\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m    740\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\saich\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_classification\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mcheck_classification_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\saich\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\multiclass.py\u001b[0m in \u001b[0;36mcheck_classification_targets\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    170\u001b[0m     if y_type not in ['binary', 'multiclass', 'multiclass-multioutput',\n\u001b[1;32m    171\u001b[0m             'multilabel-indicator', 'multilabel-sequences']:\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Unknown label type: %r\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown label type: 'unknown'"
     ]
    }
   ],
   "source": [
    "#full Data\n",
    "arr_full = data_input_full.values[:,125]\n",
    "print(arr_full)\n",
    "#list_mean_median_mode = find_mean_median_mode(arr)\n",
    "#data_frame_c_full = data_frame_full_data.copy()\n",
    "#print(list_mean_median_mode)\n",
    "\n",
    "val_full = 0.01\n",
    "acc_full = 0.0\n",
    "DT_max_acc_full = 0.0\n",
    "DT_max_val_full = 0.0\n",
    "GD_max_acc_full = 0.0\n",
    "GD_max_val_full = 0.0\n",
    "SVC_max_acc_full = 0.0\n",
    "SVC_max_val_full = 0.0\n",
    "while val_full < 1.0:\n",
    "    dataset_replaced_values[125] = np.where(dataset_replaced_values[125] > val,True,False)\n",
    "    train_data_clean_full_1 = dataset_replaced_values.values\n",
    "    X_CV_train,X_CV_eval,y_CV_train,y_CV_eval = cross_val_split_full(train_data_clean_full_1)\n",
    "    \n",
    "    #Decision Tree classfier\n",
    "    DTree_clf_full=tree.DecisionTreeClassifier()\n",
    "    DTree_clf_full.fit(X_CV_train,y_CV_train)\n",
    "    acc_full = find_accuracy_2(X_CV_train,X_CV_eval,y_CV_train,y_CV_eval,DTree_clf_full)\n",
    "    if acc_full > DT_max_acc_full:\n",
    "        DT_max_val_full = val_full\n",
    "        DT_max_acc_full = acc_full\n",
    "    \n",
    "    #GaussianNB classfier\n",
    "    Gaussian_clf_full_1 = GaussianNB()\n",
    "    Gaussian_clf_full_1.fit(X_CV_train,y_CV_train.astype(int))\n",
    "    acc_full = find_accuracy_2(X_CV_train,X_CV_eval,y_CV_train,y_CV_eval,Gaussian_clf_full_1)\n",
    "    if acc_full > GD_max_acc_full:\n",
    "        GD_max_val_full = val_full\n",
    "        GD_max_acc_full = acc_full\n",
    "    \n",
    "    #Linear SVC\n",
    "    SVC_clf_full_1 = SVC(kernel='linear')\n",
    "    SVC_clf_full_1.fit(X_CV_train,y_CV_train.astype(int))\n",
    "    acc_full = find_accuracy_2(X_CV_train,X_CV_eval,y_CV_train,y_CV_eval,SVC_clf_1)\n",
    "    if acc_full > SVC_max_acc_full:\n",
    "        SVC_max_val_full = val_full\n",
    "        SVC_max_acc = acc_full\n",
    "    del data_frame_c_full['HighCrime']\n",
    "    #del train_data_clean_1['HighCrime']\n",
    "    train_data_clean_full_1 = []\n",
    "    val_full += 0.02\n",
    "    \n",
    "print('Maximum accuracy threshold for Decistion Tree : ',round(DT_max_val_full,2), ' Maximum accuracy: ', DT_max_acc_full)\n",
    "print('Maximum accuracy threshold for Guassian NB : ',round(GD_max_val_full,2), ' Maximum accuracy: ', GD_max_acc_full)\n",
    "print('Maximum accuracy threshold for Linear SVC : ',round(SVC_max_val_full,2), ' Maximum accuracy: ', SVC_max_acc_full)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
